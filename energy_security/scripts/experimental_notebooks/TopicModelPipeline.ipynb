{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/nltk/decorators.py:68: DeprecationWarning: `formatargspec` is deprecated since Python 3.5. Use `signature` and the `Signature` object directly\n",
      "  regargs, varargs, varkwargs, defaults, formatvalue=lambda value: \"\"\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import glob\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "import pyLDAvis.gensim\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "from nltk.corpus import stopwords\n",
    "from stop_words import get_stop_words\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialise spaCy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read in CSVs of abstracts\n",
    "path = \"../data/elsevier/Elsevier_EnergyPolicy.csv\"\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elsevier = glob.glob(\"data/elsevier/*.csv\")\n",
    "wiley = glob.glob(\"data/wiley/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (<ipython-input-7-f15e0954c9cd>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-f15e0954c9cd>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    allFiles = glob.glob(\"~Document/digital_literacies/projects/vladimir/TopicModelling/data/elsevier/*.csv\") +\\\u001b[0m\n\u001b[0m                                                                                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "allElsevier = glob.glob(\"~Document/digital_literacies/projects/vladimir/TopicModelling/data/elsevier/*.csv\") \n",
    "allWiley = glob.glob(\"~Document/digital_literacies/projects/vladimir/TopicModelling/data/wiley/*.csv\") \n",
    "list_ = []\n",
    "\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0)\n",
    "    list_.append(df)\n",
    "\n",
    "data = pd.concat(list_, axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "allElsevier = glob.glob(\"/Users/au564346/Documents/research/digital_literacies/projects/vladimir/TopicModelling/data/elsevier/*.csv\") \n",
    "allWiley = glob.glob(\"/Users/au564346/Documents/research/digital_literacies/projects/vladimir/TopicModelling/data/wiley/*.csv\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/au564346/Documents/research/digital_literacies/projects/vladimir/TopicModelling/data/elsevier/Elsevier.Energy_Procedia.csv',\n",
       " '/Users/au564346/Documents/research/digital_literacies/projects/vladimir/TopicModelling/data/elsevier/Elsevier.Resources_Policy.csv',\n",
       " '/Users/au564346/Documents/research/digital_literacies/projects/vladimir/TopicModelling/data/elsevier/Elsevier_EnergyForSustainableDevelopment.csv',\n",
       " '/Users/au564346/Documents/research/digital_literacies/projects/vladimir/TopicModelling/data/elsevier/Elsevier_EnergyInternationalJournal.csv',\n",
       " '/Users/au564346/Documents/research/digital_literacies/projects/vladimir/TopicModelling/data/elsevier/Elsevier_EnergyPolicy.csv',\n",
       " '/Users/au564346/Documents/research/digital_literacies/projects/vladimir/TopicModelling/data/elsevier/Elsevier_EnergyResearch&SocialScience.csv',\n",
       " '/Users/au564346/Documents/research/digital_literacies/projects/vladimir/TopicModelling/data/elsevier/Elsevier_Resource&EnergyEconomics.csv']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allElsevier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stop_1 = get_stop_words(\"english\")\n",
    "stop_2 = stopwords.words('english')\n",
    "stopwords = list(set(stop_1 + stop_2))\n",
    "stopwords.extend([\"-PRON-\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Document preprocesing\n",
    "import time\n",
    "# Main pipeline\n",
    "processed_docs = [] \n",
    "\n",
    "n_threads = 3\n",
    "batch_size = 100\n",
    "for doc in tqdm(nlp.pipe(df[\"abstract\"][0:100], n_threads=n_threads, batch_size=batch_size), total=batch_size):\n",
    "    \n",
    "    # Named entities.\n",
    "    ents = doc.ents\n",
    "\n",
    "    # Keep only words (no numbers, no punctuation).\n",
    "    # Lemmatize tokens, remove punctuation and remove spaCy stopwords.\n",
    "    doc = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "\n",
    "    # Remove common words from a stopword list.\n",
    "    doc = [token for token in doc if token not in stopwords]\n",
    "\n",
    "    # Add named entities, but only if they are a compound of more than word. E.g. - United_States, European_Union\n",
    "    doc.extend([str(entity) for entity in ents if len(entity) > 1])\n",
    "    \n",
    "    processed_docs.append(doc)\n",
    "\n",
    "# Rename and delete\n",
    "docs = processed_docs\n",
    "del processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute bigrams & trigrams\n",
    "bigram = Phrases(docs, min_count=5)\n",
    "trigram = Phrases(bigram[docs])\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)\n",
    "    for token in trigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a dictionary representation of the documents, and filter out frequent and rare words.\n",
    "dictionary = Dictionary(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove rare and common tokens.\n",
    "max_freq = 0.8\n",
    "min_wordcount = 10\n",
    "dictionary.filter_extremes(no_below=min_wordcount, no_above=max_freq)\n",
    "# This sort of \"initializes\" dictionary.id2token.\n",
    "_ = dictionary[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Vectorize data.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inspect elements\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with topic coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Multiple random initializations\n",
    "model_list = []\n",
    "for i in range(5):\n",
    "    model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, \n",
    "            update_every=update_every, chunksize=chunksize, passes=passes, eval_every=eval_every)\n",
    "    top_topics = model.top_topics(corpus)\n",
    "    tc = sum([t[1] for t in top_topics])\n",
    "    model_list.append((model, tc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Choose most coherent model and display coherence\n",
    "model, tc = max(model_list, key=lambda x: x[1])\n",
    "print('Topic coherence: %.3e' %tc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_lda_topics(model, num_topics):\n",
    "    word_dict = {};\n",
    "    for i in range(num_topics):\n",
    "        words = model.show_topic(i, topn = 20);\n",
    "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = [i[0] for i in words];\n",
    "    return pd.DataFrame(word_dict);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "get_lda_topics(model, num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define function to present topics in a neater format\n",
    "def explore_topic(lda_model, topic_number, topn, output=True):\n",
    "    \"\"\"\n",
    "    accept a ldamodel, atopic number and topn vocabs of interest\n",
    "    prints a formatted list of the topn terms\n",
    "    \"\"\"\n",
    "    terms = []\n",
    "    for term, frequency in lda_model.show_topic(topic_number, topn=topn):\n",
    "        terms += [term]\n",
    "        if output:\n",
    "            print(u'{:20} {:.3f}'.format(term, round(frequency, 3)))\n",
    "    \n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Display neat topics\n",
    "topic_summaries = []\n",
    "print(u'{:20} {}'.format(u'term', u'frequency') + u'\\n')\n",
    "\n",
    "topic_summaries = []\n",
    "for i in range(num_topics):\n",
    "    print('Topic '+str(i)+' |---------------------\\n')\n",
    "    tmp = explore_topic(model,topic_number=i, topn=10, output=True )\n",
    "#     print tmp[:5]\n",
    "    topic_summaries += [tmp[:5]]\n",
    "    print(topic_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply user defined labels to topics\n",
    "top_labels = {0: 'Fossil fuel', 1:'Nuclear energy', 2:'Efficient consumption', 3:'Public policy', 4:'Cimate change', 5:'United States',\n",
    "             6:'Energy Security', 7:'Energy transition', 8:'Academic Research', 9:'International Development'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize pyLDAvis\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inspect model in pyLDAvis\n",
    "pyLDAvis.gensim.prepare(model, corpus, dictionary, sort_topics=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inspect model in pyLDAvis\n",
    "pyLDAvis.save_html(pyLDAvis.gensim.prepare(model, corpus, dictionary), 'lda.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "\n",
    "\n",
    "##### Everything below here is experimental. Beware dragons!\n",
    "\n",
    "---------------\n",
    "\n",
    "# Plotting topics use T-SNE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define useful functions\n",
    "\n",
    "def article_to_wordlist(article, remove_stopwords=True ):\n",
    "    '''\n",
    "    Word tokenizer for articles using NLTK\n",
    "    '''\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # 1. Remove non-letters\n",
    "    article_text = re.sub(\"[^a-zA-Z]\",\" \", article)\n",
    "    # 2. Convert words to lower case and split them\n",
    "    words = article_text.lower().split()\n",
    "    # 3. Remove stop words\n",
    "    words = [w for w in words if not w in stopwords]\n",
    "    # 4. Remove short words\n",
    "    words = [t for t in words if len(t) > 2]\n",
    "    # 5. lemmatizing\n",
    "    words = [nltk.stem.WordNetLemmatizer().lemmatize(t) for t in words]\n",
    "\n",
    "    return(words)\n",
    "\n",
    "# \n",
    "def get_doc_topic_dist(model, corpus, kwords=False):\n",
    "    '''\n",
    "    Creates an array containing all non-zero doc-topic distributions\n",
    "    '''\n",
    "    \n",
    "    top_dist =[]\n",
    "    keys = []\n",
    "\n",
    "    for d in corpus:\n",
    "        tmp = {i:0 for i in range(num_topics)}\n",
    "        tmp.update(dict(model[d]))\n",
    "        vals = list(collections.OrderedDict(tmp).values())\n",
    "        top_dist += [np.array(vals)]\n",
    "        if kwords:\n",
    "            keys += [np.array(vals).argmax()]\n",
    "\n",
    "    return np.array(top_dist), keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create tfvectorizer instance\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tvectorizer = TfidfVectorizer(input='content', analyzer = 'word', lowercase=True, stop_words='english',\\\n",
    "                                  tokenizer=article_to_wordlist, ngram_range=(1, 3), min_df=20, max_df=0.80,\\\n",
    "                                  norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create Document-Term Matrix using Tf-idf scores\n",
    "top_dist, lda_keys= get_doc_topic_dist(model, corpus, True)\n",
    "dtm = tvectorizer.fit_transform(df['abstract']).toarray()\n",
    "features = tvectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_ws = []\n",
    "for n in range(len(dtm)):\n",
    "    inds = np.int0(np.argsort(dtm[n])[::-1][:4])\n",
    "    tmp = [features[i] for i in inds]\n",
    "    \n",
    "    top_ws += [' '.join(tmp)]\n",
    "    \n",
    "df['Text_Rep'] = pd.DataFrame(top_ws)\n",
    "df['clusters'] = pd.DataFrame(lda_keys)\n",
    "df['clusters'].fillna(10, inplace=True)\n",
    "\n",
    "cluster_colors = {0: 'blue', 1: 'green', 2: 'yellow', 3: 'red', 4: 'skyblue', 5:'salmon', 6:'orange', 7:'maroon', 8:'crimson', 9:'black', 10:'gray'}\n",
    "\n",
    "df['colors'] = df['clusters'].apply(lambda l: cluster_colors[l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, perplexity=30)\n",
    "X_tsne = tsne.fit_transform(top_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['X_tsne'] =X_tsne[:, 0]\n",
    "df['Y_tsne'] =X_tsne[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook, save\n",
    "from bokeh.models import HoverTool, value, LabelSet, Legend, ColumnDataSource\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "source = ColumnDataSource(dict(\n",
    "    x = df['X_tsne'],\n",
    "    y = df['Y_tsne'],\n",
    "    color = df['colors'],\n",
    "    label = df['clusters'].apply(lambda l: top_labels[l]),\n",
    "#     msize = df['marker_size'],\n",
    "    topic_key = df['clusters'],\n",
    "   #title = df['title'],\n",
    "    content = df['Text_Rep']\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "title = 'T-SNE visualization of topics'\n",
    "\n",
    "plot_lda = figure(plot_width=1000, plot_height=600,\n",
    "                     title=title, tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n",
    "                     x_axis_type=None, y_axis_type=None, min_border=1)\n",
    "\n",
    "plot_lda.scatter(x='x', y='y', legend='label', source=source, color='color', alpha=0.8, size=10)#'msize', )\n",
    "\n",
    "# hover tools\n",
    "hover = plot_lda.select(dict(type=HoverTool))\n",
    "hover.tooltips = {\"content\": \"Title: @title, KeyWords: @content - Topic: @topic_key \"}\n",
    "plot_lda.legend.location = \"top_left\"\n",
    "\n",
    "show(plot_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#save the plot\n",
    "save(plot_lda, '{}.html'.format(title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
